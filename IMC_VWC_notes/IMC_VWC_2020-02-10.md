## LTER Information Management Committee Virtual Water Cooler
February 10, 2020

**Topic:** IMKE - Site Reviews: process and preparation

**Participants (26):** Renee Brown (MCM; moderator), Suzanne Remillard (AND; notes), M. Gastil-Buhl (MCR), Stevan Earl (CAP), Emery Boose (HFR), An Nguyen (BLE), Hap Garritt (PIE), Mary Martin (HBR), Jason Downing (BNZ), Li Kui (SBC), Yang Xia (KNZ), Miguel Leon (LUQ), Sarah Elmendorf (NWT), Corinna Gries (NTL/EDI), Greg Maurer (JRN), John Porter (VCR), Mark Gahler (NTL), Mark Servilla (EDI), Marty Downs (LNO), Sven Bohm (KBS), Liz Dobbins (NGA), Chris Turner (NGA), Dan Bahauddin (CDR), Kris Hall (SEV), Kristin Vanderbilt (FCE/EDI), Stace Beaulieu (NES)
The February water cooler next week will focus on how to prepare for a NSF site review so we will have some of our recently reviewed sites (ARC, BNZ, HBR, KBS, MCR, NWT, PIE) share some of their experience and answer any questions.  Everyone and anyone is welcome to share as they like.  There is also a repository set up by Marty where all IM's are encouraged to deposit their review materials in the site-review-sharing-2019 directory in the LTER-IMC Google Drive: 
 https://drive.google.com/drive/folders/1UWmQmHT3GF-RxfTWLizLsxe_AXMctWZt
In addition, we want to announce in advance that going forward the water coolers will be recorded for viewing and reviewing by members of the IMC. The recordings will be available only through Google Drive and retained for only six months before being deleted.
Here are the VWC connection details which ARE different from what we have used the last couple months but this will be the new standard VWC connection:
Join Zoom Meeting
https://ucsb.zoom.us/j/664969166


**Notes:**

MCR (Gastil)
OCE funded site.
The IM reviewer was an outside, non-LTER reviewer.
There are 3 documents available on Google drive that were provided to the reviewer.
The IM review took place 4 weeks before the on-site review; lasted about 5 hours during 2 zoom sessions.
Reviewer discovered an old, defunct website that they looked at, so Gastil had to point them to the current site..
The reviewer asked lots of questions and was very thorough.
During the on-site visit she asked more questions about data.
She asked about model code and data that doesn’t go into PASTA.
This was the impetus for the non-tabular working group.
Overall, the IM review went well.
Gastil was well prepared for the review, although she felt that she wasn’t.
Gastil felt the IMS Guidelines were very important and provided a framework for the review.
Reviewer was reasonable; Gastil was able to explain why a few time series datasets were not up-to-date and these were acceptable to the reviewer.
Gastil has found some “challenges” with data in DataOne. She explained these to the reviewer.

PIE (Hap)
Also provided docs on Google drive.
OCE funded site.
The IM reviewer was an outside, non-LTER reviewer.
Used the OCE guidelines and followed to a ‘T’.
Guidelines state to address DataOne model.
NSF was stringent about communication with the review team without NSF reps present.
Coordination was through VTC with reviewer and NSF.
Review was about 1 month in advance of review and was in a Zoom session that was crammed into about 3.5 hours. They looked at the website and how data is processed through the IMS.
Hap felt that he had to guide reviewer as to which materials should be looked at and are important guidelines for the IM to follow.
Hap felt he structured the review.
Hap also gave a presentation during the on-site review. Most non-IM reviewers weren’t all that interested.
Team asked about data that are not in PASTA (lidar, genome, models, etc.)
“Make sure your data is out there and there are DOI links” (Peter McCartney recommendation).
John Porter created a data search through PASTA portal that Hap used.
Review comments: Unfortunately, some grad students reported that they had problems finding data. They were very curious about communication within the LTER site. PIs are from MBL and from other universities. They wanted to know how everyone communicates. Was interested in learning about the large datasets. Wanted more automated QC processing of the data and other people involved. More transparency about how QC is done. PIs are responsible for their own QC. Wanted more clarity about how LT data are being used and integrated across the research.
Questions:
What sort of training do you provide to accommodate new grad students or researchers? There are tools available, but no formal training. Meets one on one with researchers.
PIE is starting to require registration for people to use the site which should help with following up. This should be a whole other IMKE topic!

NTW (Sarah E.)
DEB
Their review was less down in the weeds in IMS than the previous two reviews. The reviewer and Sarah did not discuss the IMS before the review. During the review, they were not able to meet individually. She felt it went well. The topic of large datasets did come up, but she explained how it is an LTER issue over all.
Grad students weren’t as familiar with the data submission process and data going into EDI.
Team asked about data science literacy.
Sarah was very careful about how they listed datasets that appeared in the science projects. They were transparent about datasets that were not available due to QC issues.

Questions:
Marty asked a general question about mapping publication DOIs to datasets and if we are doing that. Gastil said that they had a big discussion about that. Hap doesn’t have a good mechanism to do this, but he’s thought about it because Peter McCartney had mentioned it.
EDI portal allows the addition of publications to datasets. It’s a lot of work and reviewers aren’t expecting this yet.
John P feels like the community is working on connecting papers to data and that we may not need to do this.
At reviews, we are talking to scientists and not IMs. They want to know about the science and how IM supports the science. Most are not interested in the details.
Reviews should support sites as to how sites could overcome challenges that they are experience.

MCM (Renee)
Review was just 2 weeks ago. The IM review was more than 4 weeks before the actual review and this took Renee by surprise. 
OPP is the directorate, which is different than the others.
The IM reviewer was an outside reviewer that had reviewed another LTER site, so was familiar with LTER generally.
Reviewer wanted to meet right away and Renee was supported by the site PI responsible for IM.
MCM directed the review. They provided a 2-3 page document about updates to IM since the proposal and prepared a Powerpoint presentation. There was a follow-up zoom 2 weeks later and follow-up emails.
Renee gave PP presentation during review (zoom). The presentation was meant for the broader team and not in the weeds. There were some questions about the difference between MCM website and EDI portal. Also questions about long-term data and its use. The team was very interested in questions about data literacy. Renee had led a ‘data jam’ session that walked researchers through metadata forms, how to find data on the website and had researchers work on their own datasets.
The review report-out was very positive, but the official report hasn’t been received yet.

Questions from others that are having site reviews coming up?
SE: the question about connecting papers to data is not just an IM issue, but a community issue.
Liz: For new sites, how forgiving are reviewers for not fully developed IMS?
JP: Be explicit about what your challenges are; let the team help solve the problems.
Mary M: Agreed, that you need to lay out all of the issues.
Gastil: reviews are every 6 years and software lasts about 6 years; so this is always a challenge.
Renee: There was focus on the system, but they were able to explain what they were intending to do.
Hap: For new sites, the team wants to know what you plan is and whether you are making progress.
Renee: It is helpful to get your hands on the previous reviews.
Marty: Did other DEB sites have the same issue as Sarah? NO, that doesn’t seem normal.
Mary: She did point the reviewer to the guideline materials.

Extra notes from Stevan:

Gastil (MCR):

- OCE
- the MCR science coordinator, who oversees things like their bibliography, addressed the IM component of the MCR review with Gastil since that science coordinator addresses some things that were in the review instructions
- the IM review occurred four weeks ahead of the site visit
- the guidelines from the IMC were invaluable as a checklist
- the reviewer pored over the mcr website
- do not hide anything
- address challenges like mcr data in DataONE (not all data sets appear in DataONE and some are duplicated)
- review ~ 5 hours via zoom
- provided a list of data in pasta sensu as we had done for the proposal
- questions about how to map data to papers, and demonstrated how they do that, discussed how to get better results BUT this issue was not specifically in the instructions to the reviewer

Hap (PIE):

- OCE
- they put their eddy flux data in Americorp, should I look into that?
- need to be very transparent about communication with IM reviewer, NSF is strict about open, completely formal communication
- IM review was about four weeks in advance of the site visit
- review ~ 3 hours via zoom; also a presentation to the team during the site visit but very basic to spare them the details
- follow the instructions you are given to THE LETTER
- make sure that your data are out there and that they have DOI links
- provided a list of data in pasta sensu as we had done for the proposal
- post review comments:
  - at least one grad student said they could not find data; Hap not sure why or why they had not asked for help; panel mandated being more proactive in response to that complaint (Gastil had this happen also with a brand new grad student)
  - reviewers wanted to know process for communication with folks not at the site
  - reviewers wanted to see more automated qc processing of the data, and want to see more people than just Hap involved with that process
  - reviewers wanted it to be much more clear about how LT data are being used

Sarah (NWT):

- DEB
- IM review was not separate but part of the review team visit
- felt that there was less focus on details than what was described for MCR and PIE
- was explicit about addressing critiques from their proposal
- grad students were asked how much they knew about the data submission process, and the review team was surprised how little the grad students knew
- wanted to know what they were doing about data literacy
- careful that all data sets that went into the science report were archived, documented, and easy to find

Renee (MCM)

- Polar
- review was ~ six weeks before site visit, Renee was not ready for that - be ready long before your visit
- reviewer was not an LTER person but had previously reviewed an LTER site so was familiar
- ~ 1 hour review with ~ 1.5 hour follow up a week or so later, and some email communication
- leave lots of questions for the review team; lots of questions about data in EDI
- got a lot of questioning about linking data and papers
- a lot of questions about data literacy; MCR had conducted data jams for finding and submitting data, which the team really liked
- has not yet received feedback
- wanted to know about the challenges, which Renee addressed (lots of back end, server stuff in their cast)


Regarding connecting data and papers:
- Corinna: The EDI portal allows adding a paper DOI to a data set. Generally intractable to do this with historic papers/data or holistically but this capability exists and we should be more proactive about this.
- John Porter: this is extremely difficult for information managers to address, and is really an issue for the larger community to address

Miscellaneous:
- John Porter: keep in mind that the review team are not information managers, and, thus, are not necessarily interested in the details rather more interested in the big picture and how IM contributes to science
- John Porter: the review team is there to help with challenges so do not be discouraged by comments and criticisms
- Hap: they want to see progress
- Get hands on comments from the previous review if they are available address those; even better to get comments from all prior reviews to highlight the advancement and evolution of the system


CHAT:
From sven to Everyone:  12:01 PM
switching over to phone.(slow internet).
From Renée to Everyone:  12:02 PM
https://drive.google.com/drive/folders/1UWmQmHT3GF-RxfTWLizLsxe_AXMctWZt
From John Porter to Everyone:  12:03 PM
I am getting "403 - no access" to that page
From Marty Downs to Everyone:  12:04 PM
I’m checking on it John. Are you logged into an account with a linked google account?
From Marty Downs to Everyone:  12:15 PM
https://drive.google.com/drive/folders/1qOzCnPMwS6Um5X2okOdz9bdcPD2IXr-K
From M. Gastil-Buhl to Everyone:  12:17 PM
Same rules during the MCR review about no informal communications, all emails cc-ed.
From An to Everyone:  12:19 PM
interesting, I didn't know that!
From M. Gastil-Buhl to Everyone:  12:27 PM
MCR also used John Porter's pasta inventory tool with DOIs and start/end times. (Thx JP!)
They also asked us (MCR) about communication so that must be a thing.
They asked us about use of long-term data too.
Yep - again, same, at MCR there was a brand new student who had never been to campus was the one who did not know about data.
From Jason to Everyone:  12:39 PM
At BNZ we also link publications (with or without a DOI) to data sets but because it is a self reporting feature I don't know how complete the accounting is.
From M. Gastil-Buhl to Everyone:  12:40 PM
we showed our reviewer the tool at portal for tagging datasets to papers post-hoc. We also did live demos at search sites.
From An to Everyone:  12:40 PM
this would be a great topic for a future VTC
I'd love to learn more
From Miguel Leon to Everyone:  12:51 PM
how do we make a google scholar alert for dataset DOIs?
From An to Everyone:  12:52 PM
+1 for Liz's question
From stace to Everyone:  12:53 PM
Thanks everyone this has been very helpful for me to know how to prepare
From Jason to Everyone:  12:54 PM
Acknowledge your issues and share the plan you have developed to address them
From Renée to Everyone:  12:54 PM
+1 Jason
From Jason to Everyone:  12:55 PM
And share the reasoning for the decisions you have made.
From Liz Dobbins to Everyone:  12:55 PM
thanks!
From Jason to Everyone:  12:57 PM
All of my reviews have had extra IM time either in person or zoom this last year
From An to Everyone:  12:59 PM
thanks everyone!





